ğŸ“„ Chat with your PDF (Llama 2 RAG)

This is a Retrieval-Augmented Generation (RAG) chatbot built with Llama 2, allowing users to upload PDFs and ask questions based on the document content. It extracts text, converts it into vector embeddings using FAISS, and retrieves relevant sections for generating AI-powered responses.

ğŸš€ Features
âœ… Upload PDFs and extract text automatically
âœ… Retrieve relevant document chunks using FAISS
âœ… AI-powered answers using Llama 2
âœ… Streamlit UI for easy interaction
âœ… GPU acceleration for faster responses (CUDA/PyTorch)

ğŸ“Œ Installation
1ï¸âƒ£ Clone the repository

git clone https://github.com/mayank8868/rag-chatbot
cd chat-with-pdf-llama2

2ï¸âƒ£ Set up a virtual environment (optional but recommended)
python -m venv venv
venv\Scripts\activate  # On Windows

3ï¸âƒ£ Install dependencies

pip install -r requirements.txt

ğŸ“¥ Download Llama 2 Model
This project requires Llama 2 (7B Chat) GGUF model. Download it from:
ğŸ”— Meta AI Official Website

After downloading, place the model file in the models/ directory:

models/llama-2-7b-chat.Q4_K_M.gguf
ğŸƒâ€â™‚ï¸ Running the Application

1ï¸âƒ£ Process PDFs (Create Vector Store)
Run the PDF processing script to generate embeddings:
python pdf_processing.py

2ï¸âƒ£ Start the Streamlit Web App

streamlit run scripts/app.py
Access the chatbot at: http://localhost:8501

ğŸ“œ File Structure

ğŸ“‚ chat-with-pdf-llama2
â”‚-- ğŸ“‚ data
â”‚   â”‚-- ğŸ“‚ saved_pdfs          # Uploaded PDFs
â”‚   â”‚-- ğŸ“‚ vector_store        # FAISS vector database
â”‚-- ğŸ“‚ models                  # Llama 2 model files
â”‚-- app.py                     # Streamlit UI for chatbot
â”‚-- chatbot.py                 # Chatbot logic with Llama 2
â”‚-- pdf_processing.py           # Extract text & create FAISS embeddings
â”‚-- requirements.txt            # Dependencies list
â”‚-- README.md                   # Project documentation
âš™ï¸ Configuration
Modify the configuration inside app.py and chatbot.py as needed.

Parameter	Description
MODEL_PATH	Path to the Llama 2 model file
Vector Store Directory	data/vector_store/ for FAISS embeddings
GPU Acceleration	Uses CUDA if available (n_gpu_layers=-1 in Llama config)
ğŸ› ï¸ Troubleshooting
âŒ Model file not found error
Ensure the Llama 2 model file is inside the models/ directory.

ğŸ› ï¸ CUDA device not found
Check if PyTorch with CUDA is installed:


python -c "import torch; print(torch.cuda.is_available())"
If False, install the correct version of PyTorch:


pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
ğŸ“Œ Future Enhancements
âœ… Multi-PDF support
âœ… Chat history persistence
âœ… Optimize response time

ğŸ¤ Contributing
Feel free to open an issue or submit a pull request! ğŸš€

ğŸ“œ License
This project is licensed under the MIT License.
